{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b5e33d",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo Naive Bayes\n",
    "## Classificação de Acidentes de Trânsito\n",
    "\n",
    "Este notebook treina um modelo de classificação usando **Naive Bayes** para identificar:\n",
    "- Acidentes de trânsito graves\n",
    "- Acidentes de trânsito moderados  \n",
    "- Não acidentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e9136",
   "metadata": {},
   "source": [
    "## 1. Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73de020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2c318",
   "metadata": {},
   "source": [
    "## 2. Configurar Caminhos e Mapeamento de Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0704f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dataset_finalized\n",
      "Classes: ['severe_accident', 'moderate_accident', 'no_accident']\n"
     ]
    }
   ],
   "source": [
    "# Caminho para o dataset\n",
    "path_raiz = Path('dataset_finalized')\n",
    "\n",
    "# Mapeamento das classes\n",
    "classes_num = {\n",
    "    \"severe_accident\": 0,\n",
    "    \"moderate_accident\": 1,\n",
    "    \"no_accident\": 2\n",
    "}\n",
    "\n",
    "print(f\"Dataset: {path_raiz}\")\n",
    "print(f\"Classes: {list(classes_num.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec424ed",
   "metadata": {},
   "source": [
    "## 3. Função de Extração de Features\n",
    "\n",
    "Esta função extrai características das imagens usando:\n",
    "- **HOG** (Histogram of Oriented Gradients): Detecta formas e contornos\n",
    "- **Canny**: Densidade de bordas na imagem\n",
    "- **Harris**: Densidade de cantos detectados\n",
    "- **LBP** (Local Binary Pattern): Características de textura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d228de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Função de extração de features definida!\n"
     ]
    }
   ],
   "source": [
    "def extrair_features_avancadas(caminho_img):\n",
    "    \"\"\"Extrai features da imagem usando HOG, bordas, cantos e textura\"\"\"\n",
    "    img = cv2.imread(str(caminho_img), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "\n",
    "    # HOG\n",
    "    features_hog = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                       cells_per_block=(2, 2), visualize=False)\n",
    "\n",
    "    # Bordas\n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    densidade_bordas = np.array([np.sum(edges > 0) / edges.size])\n",
    "\n",
    "    # Cantos\n",
    "    dst = cv2.cornerHarris(img, 2, 3, 0.04)\n",
    "    densidade_cantos = np.array([np.sum(dst > 0.01 * dst.max()) / dst.size])\n",
    "\n",
    "    # Textura\n",
    "    lbp = local_binary_pattern(img, 8, 1, method=\"uniform\")\n",
    "    hist_lbp, _ = np.histogram(lbp.ravel(), bins=10, range=(0, 10), density=True)\n",
    "\n",
    "    return np.hstack([features_hog, densidade_bordas, densidade_cantos, hist_lbp])\n",
    "\n",
    "print(\"Função de extração de features definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92cbdf",
   "metadata": {},
   "source": [
    "## 4. Carregar e Processar Imagens do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865b08a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extração de features do dataset...\n",
      "Processando pasta: moderate_accident\n",
      "   240 imagens processadas\n",
      "Processando pasta: no_accident\n",
      "   240 imagens processadas\n",
      "Processando pasta: severe_accident\n",
      "   240 imagens processadas\n",
      "\n",
      "Total de imagens carregadas: 720\n"
     ]
    }
   ],
   "source": [
    "dados_lista = []\n",
    "labels_lista = []\n",
    "\n",
    "print(\"Iniciando extração de features do dataset...\")\n",
    "\n",
    "for folder_path in path_raiz.iterdir():\n",
    "    if folder_path.is_dir():\n",
    "        nome_categoria = folder_path.name.replace(\"dataset_final_\", \"\")\n",
    "        \n",
    "        if nome_categoria in classes_num:\n",
    "            rotulo = classes_num[nome_categoria]\n",
    "            print(f\"Processando pasta: {nome_categoria}\")\n",
    "            \n",
    "            arquivos = list(folder_path.glob('*'))\n",
    "            contador = 0\n",
    "            \n",
    "            for path_arquivo in arquivos:\n",
    "                if path_arquivo.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                    features = extrair_features_avancadas(path_arquivo)\n",
    "                    if features is not None:\n",
    "                        dados_lista.append(features)\n",
    "                        labels_lista.append(rotulo)\n",
    "                        contador += 1\n",
    "            \n",
    "            print(f\"   {contador} imagens processadas\")\n",
    "\n",
    "print(f\"\\nTotal de imagens carregadas: {len(dados_lista)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70116b8",
   "metadata": {},
   "source": [
    "## 5. Preparar Dados para Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b32ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dos dados: (720, 8112)\n",
      "Shape dos labels: (720,)\n",
      "Número de features: 8112\n",
      "\n",
      "Dados divididos:\n",
      "   Treino: 576 amostras\n",
      "   Teste: 144 amostras\n"
     ]
    }
   ],
   "source": [
    "X = np.array(dados_lista)\n",
    "y = np.array(labels_lista)\n",
    "\n",
    "print(f\"Shape dos dados: {X.shape}\")\n",
    "print(f\"Shape dos labels: {y.shape}\")\n",
    "print(f\"Número de features: {X.shape[1]}\")\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nDados divididos:\")\n",
    "print(f\"   Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"   Teste: {X_test.shape[0]} amostras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977a120",
   "metadata": {},
   "source": [
    "## 6. Normalizar Dados com StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f144bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados normalizados com StandardScaler\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Dados normalizados com StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c620e2c",
   "metadata": {},
   "source": [
    "## 7. Treinar o Modelo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef41d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "modelo = GaussianNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "print(\"Modelo treinado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d45be6",
   "metadata": {},
   "source": [
    "## 8. Avaliar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03bb792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de classificação:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  severe_accident       0.72      0.82      0.77        40\n",
      "moderate_accident       0.80      0.73      0.77        56\n",
      "      no_accident       0.70      0.69      0.69        48\n",
      "\n",
      "         accuracy                           0.74       144\n",
      "        macro avg       0.74      0.75      0.74       144\n",
      "     weighted avg       0.75      0.74      0.74       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "print(\"Relatório de classificação:\")\n",
    "print(classification_report(y_test, y_pred, target_names=list(classes_num.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ace11",
   "metadata": {},
   "source": [
    "## 9. Salvar Modelo e Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a79b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo em: ..\\website\\ml_models\\modelo_ia.pkl\n",
      "Scaler salvo em: ..\\website\\ml_models\\scaler.pkl\n",
      "\n",
      "Treinamento concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path('../website/ml_models')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = output_dir / 'modelo_ia.pkl'\n",
    "scaler_path = output_dir / 'scaler.pkl'\n",
    "\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(modelo, f)\n",
    "\n",
    "with open(scaler_path, \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"Modelo salvo em: {model_path}\")\n",
    "print(f\"Scaler salvo em: {scaler_path}\")\n",
    "print(f\"\\nTreinamento concluído com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
